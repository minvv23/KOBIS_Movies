{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "from datetime import date, datetime, timedelta\n",
    "import requests, urllib.request, urllib.parse, csv, re, time, os, numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daum_url(start, end, query, press_name) :\n",
    "    press_id_lists = {'경향신문':'16bfGN9mQcFhOx4F5l',\n",
    "                  '한겨레':'16CIYSC5zGTVsMKcxM',\n",
    "                  '조선일보':'16EeZKAuilXKH5dzIt',\n",
    "                  '중앙일보':'16nfco03BTHhdjCcTS',\n",
    "                  '동아일보':'16Et2OLVVtHab8gcjE',\n",
    "                  '국민일보':'16NwX_ox536G_zyJUF',\n",
    "                  '한국일보':'16hsvX4VEJdcIZzt_z',\n",
    "                  '한국경제':'16qCuwnoTf8fLmrhD1',\n",
    "                  '매일경제':'16jCK_TdtzwnmXfznB'}\n",
    "    pre_url = \"https://search.daum.net/search?w=news&sort=recency&q=%s&cluster=n&s=NS&a=STCF&dc=STC&pg=1&r=1&p=%d&rc=1&at=more&sd=%d000000&ed=%d235959&period=u&cp=%s&cpname=%s\"\n",
    "    url = pre_url %(urllib.parse.quote(query),\n",
    "                    1,\n",
    "                    start,\n",
    "                    end,\n",
    "                    press_id_lists[str(press_name)],\n",
    "                    urllib.parse.quote(press_name))\n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daum_url_pagenum(start, end, query, press_name, page_num) :\n",
    "    press_id_lists = {'경향신문':'16bfGN9mQcFhOx4F5l',\n",
    "                  '한겨레':'16CIYSC5zGTVsMKcxM',\n",
    "                  '조선일보':'16EeZKAuilXKH5dzIt',\n",
    "                  '중앙일보':'16nfco03BTHhdjCcTS',\n",
    "                  '동아일보':'16Et2OLVVtHab8gcjE',\n",
    "                  '국민일보':'16NwX_ox536G_zyJUF',\n",
    "                  '한국일보':'16hsvX4VEJdcIZzt_z',\n",
    "                  '한국경제':'16qCuwnoTf8fLmrhD1',\n",
    "                  '매일경제':'16jCK_TdtzwnmXfznB'}\n",
    "    pre_url = \"https://search.daum.net/search?w=news&sort=recency&q=%s&cluster=n&s=NS&a=STCF&dc=STC&pg=1&r=1&p=%d&rc=1&at=more&sd=%d000000&ed=%d235959&period=u&cp=%s&cpname=%s\"\n",
    "    url = pre_url %(urllib.parse.quote(query),\n",
    "                    page_num,\n",
    "                    start,\n",
    "                    end,\n",
    "                    press_id_lists[str(press_name)],\n",
    "                    urllib.parse.quote(press_name))\n",
    "    return(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daum_news_scraper(start, end, query, press_name) :\n",
    "    news_links = []\n",
    "    date_list = [datetime.strptime(str(end),'%Y%m%d') - timedelta(days=x) \n",
    "             for x in range(0, (datetime.strptime(str(end),'%Y%m%d')-datetime.strptime(str(start),'%Y%m%d')).days+1)]\n",
    "    session = requests.Session()\n",
    "    header = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)\\AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "              \"Accept\":\"text/html,application/xhtml+xml,application/xml;\\q=0.9,imgwebp,*/*;q=0.8\"}\n",
    "    url = daum_url(start, end, query, press_name)\n",
    "    req = session.get(url, headers=header)\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    total_num_article = int(''.join(re.findall('\\d+',re.search(r'/ (.*?)건',soup.find('span', attrs={'class':'txt_info'}).text).group(1))))\n",
    "    \n",
    "    if total_num_article <= 800 :\n",
    "        for each in range(1,(total_num_article//10)+2) :\n",
    "            time.sleep(.25)\n",
    "            new_soup = BeautifulSoup(session.get(daum_url_pagenum(start, end, query, press_name, each), headers=header).text, 'html.parser')\n",
    "            for each_link in new_soup.find_all('div', attrs={'class':'wrap_tit mg_tit'}) :\n",
    "                news_links.append(each_link.a['href'])\n",
    "                clear_output(wait=True)\n",
    "                print(len(news_links),'/',total_num_article)                \n",
    "        \n",
    "    else :\n",
    "        for each_date in date_list :\n",
    "            time.sleep(.25)\n",
    "            new_url = daum_url(int(each_date.strftime('%Y%m%d')), int(each_date.strftime('%Y%m%d')), query, press_name)\n",
    "            new_req = session.get(new_url, headers=header)\n",
    "            new_soup = BeautifulSoup(new_req.text, 'html.parser')\n",
    "            num_article = int(''.join(re.findall('\\d+',re.search(r'/ (.*?)건',new_soup.find('span', attrs={'class':'txt_info'}).text).group(1))))\n",
    "\n",
    "            if num_article == 0 :\n",
    "                pass\n",
    "            else :\n",
    "                for each_page in range(1,(num_article//10)+2) :\n",
    "                    time.sleep(.25)\n",
    "                    new_soup2 = BeautifulSoup(session.get(\n",
    "                        daum_url_pagenum(int(each_date.strftime('%Y%m%d')), int(each_date.strftime('%Y%m%d')), query, press_name, each_page), headers=header).text, 'html.parser')\n",
    "                    for each_link in new_soup2.find_all('div', attrs={'class':'wrap_tit mg_tit'}) :\n",
    "                        news_links.append(each_link.a['href'])\n",
    "                        clear_output(wait=True)\n",
    "                        print(len(news_links),'/',total_num_article)\n",
    "    print(len(news_links),'/',total_num_article, 'COMPLETE!')\n",
    "    pd.DataFrame(news_links).to_csv(str(start)+'_'+str(end)+'_'+query+'_'+press_name+'.csv')\n",
    "    return(news_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
